{"cells":[{"cell_type":"markdown","source":"","metadata":{"cell_id":"00000-f2227a60-9b19-4832-b1f8-218399400879","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# AM207 Final Paper - Decomposition of Uncertainty in Bayesian Deep Learning","metadata":{"cell_id":"00001-f626a932-3314-4e12-8221-9700d34f9fc9","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Team Members - **add research goal for Spring if wanted**\n\n* Owen Callen\n* Gabriel Pestre\n* Hayden Sansum\n* Nikhil Vanderklaauw","metadata":{"cell_id":"00002-35979f6d-d689-46ef-a76b-00b10a3e0ae5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Problem Statement\n\nThe authors of this paper \\[1\\] propose a method of utilizing the flexibility of a Bayesian Neural Network with Latent Variables to allow for the decomposition of Aleatoric and Epistemic Uncertainty. When performing supervised learning or reinforcement learning there are a number of ways to gain insight into the overall uncertainty, such as frequentist bootstrapping (running multiple trials) or implementing a Bayesian modeling approach to incorporate a distribution over models. Both these approaches however only provide the overall magnitude of the uncertainty and do not allow for further interrogation of where those uncertainties might be coming from. Without knowing how much of the variation is due to inherent randomness (aleatoric) or due to the lack of collected data in specific areas (epistemic) there is no way to know precisely how to improve the model. For instance would it be useful to collect more data in a certain area? Would it be useful to defer the prediction to an expert? In reinforcement learning this could be framed as, where does my agent need to explore more to maximize how much it learns about the environment? Being able to decompose uncertainty into these two core components, epistemic and aleatoric (whilst still using a complex and flexible model such as a neural network) enables practioners of data science and statistics to tackle these problems. \n\nAside from this primary aim, of decomposing uncertainty, the authors are also interested in proving that the abililty to achieve this decomposition of uncertainty in this way is an inherent property of the Bayesian Neural Network with Latent Variables (BBN+LV) (originally presented in an early paper by the author \\[2\\]) and not an artefact of the sampling methodology. As such they attempt to show that the BNN+LV has the properties required to split epistemic from aleatoric uncertainty when using both BB-alpha (a similar method to BBVI although using an alpha divergence) and also Hamiltonian Monte Carlo (HMC).","metadata":{"cell_id":"00003-71a285ce-f760-4512-aafb-853f2443e006","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Context/Scope\n\n* Why is this important?\n* Why does splitting aleatoric from epistemic uncertainty matter?\n* How have they framed it in terms of heteroscedastic vs bimodal noise\n* What about latent variables - baby weights etc.\n* Reinforcement learning - active learning - where to explore\n* Novel risk sensitive criterion?","metadata":{"cell_id":"00004-1999f269-c004-4c2f-8401-000ae86fd207","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Existing Work\n\n* **Our paper**: Depeweg - Decomposition of Uncertainty in Bayesian Deep Learning for Efficient Risk-sensitive learning (https://arxiv.org/pdf/1710.07283.pdf)\n  * Self explanatory - this is our main paper\n\n* BNN+LV paper: Depeweg - Learning and Policy Search in Stochastic Dynamical Systems With Baysian Neural Networks (https://arxiv.org/pdf/1605.07127.pdf)\n  * This is the original Depeweg paper where they introduce the idea of using BNN+LV in the first place\n\n* Overcoming non-identifiability in BNN+LVs: Wei Wei Pan - Learning Deep Bayesian Latent Variable Regression Models that\nGeneralize: When Non-identifiability is a Problem (https://arxiv.org/pdf/1911.00569.pdf)\n  * A later paper by Prof Pan and a few others which highlights some of the problems with the original BNN+LV uncertainty paper in that its almost impossible to train\n\n* Sensitivity Analysis: Depeweg - Sensitivity Analysis for Predictive Uncertainty\nin Bayesian Neural Networks (https://arxiv.org/pdf/1712.03605.pdf)\n  * Depeweg also published a paper looking into the sensitivity of the results of the orignal paper so we should pull out of few key findings from here\n\n* Bayes by Backprop (BBN - BBVI): Blundell - Weight Uncertainty in Neural Networks (https://arxiv.org/pdf/1505.05424.pdf)\n  * The OG BBVI for NN implementation - we should mention and credit this as the paper which enabled use to perform approximate inference on Neural Network posteriors\n\n* Applications of the decomposition in image detection: Kendall - What Uncertainties Do We Need in Bayesian Deep\nLearning for Computer Vision? (https://arxiv.org/pdf/1703.04977.pdf)\n  * A specific application of the BBN_LV models to CV - i think they expand on the toy examples with some real heteroscedastic noise examples\n\n* Uncertainty through Entropy Decomposition: Kozachenko - Sample Estimate of the Entropy of a Random Vector (http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=ppi&paperid=797&option_lang=eng)\n  * The old paper demonstrating how you can use entropy to get uncertainty in a dataset (its in Russian but we only used the maths)\n\n* A newer easy to digest version of the above with additional details we ignorewd: Singh - Nearest Neighbor Estimates of Entropy (https://www.tandfonline.com/doi/pdf/10.1080/01966324.2003.10737616?needAccess=true)\n\n\nInterestingly, in our search for existing literature on the use of Entropy in decomposing uncertainties into epistemic and aleatoric components, \na number of engineering papers had already leveraged the approach. Robertson et al's paper on Wind Turbine uncertainty \\[9\\] separated variables into modeling variables\nand natural parameters (with the modeling variables capturing the epistemic and natural parameters capturing the aleatoric uncertainty). In a way this \nis a parallel to how Depeweg et al. splits out epistemic as uncertainty over modeling weights and aleatoric as the remaining uncertainty inherent in the data. \nGong et al's research into hydrolic modeling \\[10\\] lays out this concept in a more general manner where the aleatoric uncertainty is what remains after \nattributing some component of the total uncertainty to the mutual information between the data and the output (epistemic).","metadata":{"cell_id":"00005-c5c307cd-22b1-47d4-927a-f4db73137004","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Contribution\n\nThe contribution of this paper is threefold and centers on the uniqueness of utilizing a BNN+LV with uncertainty decompositions:\n\n1. The specific methods (described below in our technical section) of decomposing the predictive uncertainty of a BNN+LV into epistemic and aleatoric \nuncertainties by using entropy and variance. The algorithm used to calculate the entropy of a set of data has been around since the original Kozachenko \npaper in 1987 \\[8\\] and the overall approach, as mentioned above, of using entropy as a method to decompose uncertainty was proposed and utilized in \nengineering at least back as far as 2013. It is in this context however, applying the decomposition methods to a BBN+LV and utilizing that to be able to \nseparate the aleatoric and epistemic compnents of a neural network that are a unique contribution.\n\n2. Demonstrating the ability of the BBN+LV, when applied with the decomposition of uncertainty approach, to separate out epistemic and aleatoric\nuncertainties from even very complex functions such as those with bimodal or heteroscedastic. This innovation allows for practictioners to identify areas\nof high epistemic noise, even for very difficult to learn functions, and therefore train models efficiently by gathering more data or further training the model in\nthese areas.\n\n3. Im confused about this one - NOVEL RISK SENSITIVE CRITERION - need to research more TODO...","metadata":{"cell_id":"00006-48737d79-23be-4456-9c0d-ba41d0a2ea0d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Technical Content\n\nIn this section we will summarize the core technical context required to understand the methodology presented in the paper and any additional methods we attempted in replicating the paper. \nDetails on our specific implementation can be found in the experiments section of this paper.","metadata":{"cell_id":"00007-473651e8-63f6-4e5b-9049-7da4c82f861e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### BBN+LV\n\n* this is our main piece of machinery - explain why its useful and how it works ","metadata":{"cell_id":"00008-6fd3411d-e6af-42b1-a171-29df39647a99","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Entropy Decomposition ","metadata":{"cell_id":"00009-54160b01-9883-48b9-a310-90c9ced9692e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"To distinguish between the epistemic and aleatoric uncertainties, the authors required a method to decompose where the uncertainties are being introduced - either the uncertainties across the weights (epistemic) or the uncertainty inherent within the noise of the data (aleatoric). \n\nIn the original Depeweg paper \\[1\\] the authors proposed two ways to perform this estimation, via entropy and via variance. In the results section of the paper, the authors only present and discuss their results in terms of the entropy and so this is the method that we will focus on reproducing. The entropy of a set of points represent how much inherent variation exists. Formally this can be calculated using the functional pdf, but in this case where our BNN+LV posterior is not a known form, it must be approximated using a technique such as K-Nearest Neighbor.\n\nUsing KNN as an approximate Entropy calculation goes back to a 1987 paper by Kozachenko & Leonenko \\[7\\]\nalthough is more succinctly stated in a later 2003 paper by Singh et al \\[8\\] with the relevant equation being listed as (Eqn. 4). Entropy as a concept is actually a measurement of randomness and is leverged in many fields from thermodynamics to algorithms which optimize splits in a decision tree. In this specific case, the randomness (entropy) that is interesting is how much difference there is in the predictions at each input data point. K-Nearest Neighbors is therefore an obvious candidate for an approximation as it enables an efficient and fast method to calculate distances between a vector of datapoint (a set of $y*$ predictions for a given $x*$). \n\nThis KNN approximation of the Entropy, in general form, is stated as:\n$$ \n\\hat{H}_n = \\frac{p}{n}\\sum_{i=1}^n ln(p_i) + ln\\left[ \\frac{\\pi^{p/2}}{\\Gamma(\\frac{p}{2} + 1)} \\right] + \\gamma + ln(n-1)\n$$\n\nWhere:\n$$ \np = \\text{dimensionality} \\quad n = \\text{number of data points} \\quad p_i = \\text{distance to nearest neighbor} \\quad \\gamma = \\text{Euler constant}\n$$\n\nSo in essence the entropy is the average of the natural log of the distance from each point to its closest neighbour, scaled by the size and dimensionality of the problem at hand. The additional constants are related to the surface area of a hypersphere in $p$ dimensions and scales the entropy based on the overall size of the hypersphere within which the entropy is being calculated. \n\nSingh et al [\\8\\] also propose more robust methods of approximation utilizing more than just the single nearest neighbouring point but for simplicity of reproduction and to more closely match the Depeweg paper \\[1\\], which cites the original Kozachenko paper \\[8\\], we will stick to the single nearest neighbor method.","metadata":{"cell_id":"00010-11c96fc4-79fb-4745-958f-85395a17e47a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"With a method for approximating the Entropy of a given set of data, we must then apply this to the BNN+LV in order to decompose the uncertainty. Naively applying the entropy calculation across the outputs of the BNN+LV however will not give us anything more than the overall uncertainty, including both aleatoric and epistemic. To decompose this we need to calculate two entropies, the overall entropy and the entropy which occurs having marginalized out the uncertainty over the weights (aleatoric uncertainty). The difference of which leaves just the epistemic uncertainty.\n\nThe formulation given in the paper for this decomposition is:\n$$\n\\text{Overall uncertainty} = H(y*|x*)\n$$\n$$\n\\text{Aleatoric uncertainty} = \\mathbb{E}_{q(W)}[H(y*|W,x*)]\n$$\n$$\n\\text{Epistemic uncertainty} = H(y*|x*) - \\mathbb{E}_{q(W)}[H(y*|W,x*)]\n$$\n\nBeing unable to calculate the entropies exactly though, we instead substitute in our approximation for the entropy, $\\hat{H}(.)$ and calculate it as follows:\n$$\n\\text{Overall uncertainty} = \\hat{H}(y*^1,...,y*^L)\n$$\n$$\n\\text{Aleatoric uncertainty} = \\frac{1}{M} \\sum_{i=1}^M \\hat{H}(y*^{1,W_i}, ..., y*^{L,W_i})\n$$\n$$\n\\text{Epistemic uncertainty} = \\hat{H}(y*^1,...,y*^L) - \\frac{1}{M} \\sum_{i=1}^M \\hat{H}(y*^{1,W_i}, ..., y*^{L,W_i})\n$$\n\nWhere:\n$$ \nL = \\text{number of data points} \\quad M = \\text{number of different models (posterior samples)} \n$$\n\nIn simpler terms:\n\n1. Use the KNN formula to calculate the overall entropy of a given set of output data points, $y*$ from all the models at once, this treats all predictions from each model, at a specific data point $x*$ as a single vector. This calculates the overall entropy present in the predictions.\n2. Make multiple predictions using a single set of model weights, $W_i$ and calculate the entropy at each $y*$ within this model. Repeat this process for every different model individually ($W_i,...,W_M$) and average the entropy across these outcomes. By separately calculating the average of the entropy across many different models through repeated sampling of the weights, this averages out the effects of the uncertainty between models (aka marginalizes out the epistemic uncertainties leaving just the aleatoric).\n3. Finally summarize the Epistemic uncertainty as $Total - Aleatoric$","metadata":{"cell_id":"00011-b74268f5-33ee-4310-bb74-24983dc8d5b4","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### BBVI\n\n* Why do we need BBVI - generally cover why we care about sampling from an approximate posterior then cover the BBVI implementation\n* Talk about the ELBO - highlight where it differs from normal BBVI for BNN\n* Highlight our posterior derivation here\n* State our priors\n* State our likelihood\n* Lots of math and equations would be good here","metadata":{"cell_id":"00012-666e94c8-83ca-4521-9151-aa078bdbf8c0","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### BB-alpha\n\nJust a high level overview - nothing too indepth as we arent going to implement it\n* how does it differ from BBVI\n* Why is it more appropriate for this task? Multimodal vs unimodal posterior approximation","metadata":{"cell_id":"00013-e585703a-7419-4370-a514-2c863e614d7c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### HMC & PYMC3\n\n* Talk first about sampling from posterior rather than an approximation to the posterior\n* This is still on approximate solution though as it never really converged to the true posterior\n* Same posterior set up as BBVI? Do we need to cover something different here in terms of posterior?\n* How does it work and why did we try it?\n\nPYMC3\n* Why did we try this in addition?\n* What is NUTS?\n* How does it work and how does it differ to our ordinary HMC?","metadata":{"cell_id":"00014-6821e06e-4ca2-45ea-b213-736f9ce2c771","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Experiments\n\nWe need to HIGHLIGHT:\n* At least one instance of it working - aka we can kinda replicate the results and explain how we did it in detail and why\n* At least one instance of it failing - where did we find it difficult to replicate or run into problems?","metadata":{"cell_id":"00015-d638a679-82b1-4a62-aa66-e082a4260af2","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Heteroscedastic Noise\n\n* Introduction to why this toy example is important - real life situations\n* What specifically is the set up here? (parameters etc.)\n* Lay out the different methods we applied to it\n\n#### Results\n\n* Present and discuss each of the methods we used to try to decompose the uncertainty and what we found\n\n#### Discussion\n\n* How close were our results to theirs?\n* Why?\n* Anything worthy to note?","metadata":{"cell_id":"00016-47386524-ec01-4e9a-b752-8602b3b94dc0","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Bimodal Noise\n\n* Introduction to why this toy example is important - real life situations\n* What specifically is the set up here? (parameters etc.)\n* Lay out the different methods we applied to it\n\n#### Results\n\n* Present and discuss each of the methods we used to try to decompose the uncertainty and what we found\n\n#### Discussion\n\n* How close were our results to theirs?\n* Why?\n* Anything worthy to note?","metadata":{"cell_id":"00017-d318e75b-f7b5-41d5-a90b-85a1d2018887","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Wet Chicken Dynamics\n\n* Introduction to why this toy example is important - real life situations\n* What specifically is the set up here? (parameters etc.)\n* Lay out the different methods we applied to it\n\n#### Results\n\n* Present and discuss each of the methods we used to try to decompose the uncertainty and what we found\n\n#### Discussion\n\n* How close were our results to theirs?\n* Why?\n* Anything worthy to note?","metadata":{"cell_id":"00018-bb67fc94-e0e1-4c5a-9459-ec9ae0d6029d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Comparing Results\n\nSummarise the previous section in terms of what we found\n* What are our high level takeaways in terms of our results\n* How close did we get to replicating?\n* Does it work?","metadata":{"cell_id":"00019-d6def4d6-6a11-4019-9b8a-17ed58890bfd","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Evaluation\n\nMore about critiquing their paper\n* From our investigations where are the flaws?\n* Do we believe their results?\n* Where would this approach fail?","metadata":{"cell_id":"00020-ea2e0384-ff4a-436e-a81a-fde04c56052c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Future Work & Improvements\n\n* What should we/they investigate further if we were to do a paper on this?","metadata":{"cell_id":"00021-a4681a8a-b787-4382-97cc-dd375b8353a4","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Broader Impact \n\nEthic component - not sure what need to go here\n* Possible misuses of this technology or concept?\n* Where might it be used in real life?","metadata":{"cell_id":"00022-9288ab80-5b37-4ca9-aa5e-eab8998f7061","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## References\n\n\\[1\\] Depeweg et al (2018). Decomposition of Uncertainty in Bayesian Deep Learning for Efficient Risk-sensitive learning. arXiv:1710.07283v4. Available at: [https://arxiv.org/pdf/1710.07283.pdf]\n\n\n\\[2\\] Depeweg et al (2017). Learning and Policy Search in Stochastic Dynamical Systems With Baysian Neural Networks. arXiv:1605.07127v3. Available at: [https://arxiv.org/pdf/1605.07127.pdf]\n\n\\[3\\] Yacoby, Pan and Doshi-Velez (2019). Learning Deep Bayesian Latent Variable Regression Models that Generalize: When Non-identifiability is a Problem. arXiv:1911.00569. Available at: [https://arxiv.org/pdf/1911.00569.pdf]\n  \n\\[4\\] Depeweg et al (2017). Sensitivity Analysis for Predictive Uncertainty in Bayesian Neural Networks. arXiv:1712.03605v1. Available at: [https://arxiv.org/pdf/1712.03605.pdf]\n\n\\[5\\] Blundell et al (2015). Weight Uncertainty in Neural Networks. arXiv:1505.05424v2. Available at: [https://arxiv.org/pdf/1505.05424.pdf]\n  \n\\[6\\] Kendall and Gal (2017). What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? arXiv:1703.04977v2. Available at: [https://arxiv.org/pdf/1703.04977.pdf]\n  \n\\[7\\] Kozachenko and Leonenko (1987). Sample Estimate of the Entropy of a Random Vector. Probl. Peredachi Inf., 1987, Volume 23, Issue 2, Pages 9–16. Available at: [http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=ppi&paperid=797&option_lang=eng]\n\n\\[8\\] Singh et al (2003). Nearest Neighbor Estimates of Entropy. American Journal of Mathematical and\nManagement Sciences. 23:3-4, 301-321. Available at: [https://doi.org/10.1080/01966324.2003.10737616]\n\n\\[9\\] Robertson et al (2014). Quantifying Epistemic and Aleatoric Uncertainty in the Ampair 600 Wind Turbine. SAND2014-20179C. Available at: [https://www.osti.gov/servlets/purl/1242753]\n\n\\[10\\] Gong et al (2013). Estimating epistemic and aleatory uncertainties during hydrologic modeling: An information theoretic approach. Water Resources Research, Volume 49, Issue 4, Pages 2253-2273. Available at: [https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/wrcr.20161]","metadata":{"cell_id":"00023-c919c376-1830-4b39-9aac-a3b0b468a33a","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00024-64e1212c-3096-4bfa-992a-797a95a11db9","deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9-final"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"python_207","language":"python"},"deepnote_notebook_id":"0f50d17e-723e-48d0-9306-6d45d4ae006b","deepnote_execution_queue":[]}}