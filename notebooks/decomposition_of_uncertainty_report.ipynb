{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# AM207 Final Paper - Decomposition of Uncertainty in Bayesian Deep Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Team Members - **add research goal for Spring if wanted**\n",
    "\n",
    "* Owen Callen\n",
    "* Gabriel Pestre\n",
    "* Hayden Sansum\n",
    "* Nikhil Vanderklaauw"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Problem Statement\n",
    "\n",
    "* What is the paper attempting to solve?\n",
    "\n",
    "* Taking the flexibility of BBNs in terms of modeling complex non-linear functions but also incorporate the latent variables ability to represent complex, unknown noise present in the input. \n",
    "* By doing so demonstrate their usefulness in decomposing aleatoric from epistemic uncertainty\n",
    "* Demonstrate the usefulness of BBN+LV (independent of sampling method, BBVI, BBalpha, HMC etc.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Context/Scope\n",
    "\n",
    "* Why is this important?\n",
    "* Why does splitting aleatoric from epistemic uncertainty matter?\n",
    "* How have they framed it in terms of heteroscedastic vs bimodal noise\n",
    "* What about latent variables - baby weights etc.\n",
    "* Reinforcement learning - active learning - where to explore\n",
    "* Novel risk sensitive criterion?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Existing Work\n",
    "\n",
    "* **Our paper**: Depeweg - Decomposition of Uncertainty in Bayesian Deep Learning for Efficient Risk-sensitive learning (https://arxiv.org/pdf/1710.07283.pdf)\n",
    "  * Self explanatory - this is our main paper\n",
    "\n",
    "* BNN+LV paper: Depeweg - Learning and Policy Search in Stochastic Dynamical Systems With Baysian Neural Networks (https://arxiv.org/pdf/1605.07127.pdf)\n",
    "  * This is the original Depeweg paper where they introduce the idea of using BNN+LV in the first place\n",
    "\n",
    "* Overcoming non-identifiability in BNN+LVs: Wei Wei Pan - Learning Deep Bayesian Latent Variable Regression Models that\n",
    "Generalize: When Non-identifiability is a Problem (https://arxiv.org/pdf/1911.00569.pdf)\n",
    "  * A later paper by Prof Pan and a few others which highlights some of the problems with the original BNN+LV uncertainty paper in that its almost impossible to train\n",
    "\n",
    "* Sensitivity Analysis: Depeweg - Sensitivity Analysis for Predictive Uncertainty\n",
    "in Bayesian Neural Networks (https://arxiv.org/pdf/1712.03605.pdf)\n",
    "  * Depeweg also published a paper looking into the sensitivity of the results of the orignal paper so we should pull out of few key findings from here\n",
    "\n",
    "* Bayes by Backprop (BBN - BBVI): Blundell - Weight Uncertainty in Neural Networks (https://arxiv.org/pdf/1505.05424.pdf)\n",
    "  * The OG BBVI for NN implementation - we should mention and credit this as the paper which enabled use to perform approximate inference on Neural Network posteriors\n",
    "\n",
    "* Applications of the decomposition in image detection: Kendall - What Uncertainties Do We Need in Bayesian Deep\n",
    "Learning for Computer Vision? (https://arxiv.org/pdf/1703.04977.pdf)\n",
    "  * A specific application of the BBN_LV models to CV - i think they expand on the toy examples with some real heteroscedastic noise examples\n",
    "\n",
    "* Uncertainty through Entropy Decomposition: Kozachenko - Sample Estimate of the Entropy of a Random Vector (http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=ppi&paperid=797&option_lang=eng)\n",
    "  * The old paper demonstrating how you can use entropy to get uncertainty in a dataset (its in Russian but we only used the maths)\n",
    "\n",
    "* A newer easy to digest version of the above with additional details we ignorewd: Singh - Nearest Neighbor Estimates of Entropy (https://www.tandfonline.com/doi/pdf/10.1080/01966324.2003.10737616?needAccess=true)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Contribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Technical Content"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### BBN+LV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Entropy and Variance Decomposition "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### BBVI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### BB-alpha"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### HMC & PYMC3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Heteroscedastic Noise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Bimodal Noise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Wet Chicken Dynamics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Comparing Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Evalutation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Future Work & Improvements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Broader Impact "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}